{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "758218e8-b10f-4471-9f51-c4e21cdecc9a",
   "metadata": {},
   "source": [
    "# Harry Potter\n",
    "\n",
    "A step-by-step guide to fine tune a pre-trained language model from Hugging Face using TensorFlow and Keras on the Harry Potter books.\n",
    "\n",
    "This model could be able to run tasks like text analysis, sentiment analysis, questions answering, topic modeling, named entity recognition, and even generating Harry Potter fan fiction using techniques like natural language processing (NLP)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755470e9-2b93-42d5-8674-12c3d97befab",
   "metadata": {},
   "source": [
    "## Step 1: Install Required Libraries\n",
    "\n",
    "First, you need to install the necessary libraries. You can do this using pip.\n",
    "\n",
    "```\n",
    "!pip install transformers tensorflow datasets\n",
    "```\n",
    "\n",
    "These dependencies should be already installed. To prevent the following error:\n",
    "\n",
    "```\n",
    "RuntimeError: Failed to import transformers.models.gpt2.modeling_tf_gpt2 because of the following error (look up to see its traceback):\n",
    "Your currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.\n",
    "```\n",
    "\n",
    "Install the `tf-keras` package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46e7d10c-7691-4e37-9672-3b7c35cb31e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tf-keras in /Users/spaccs01/workspace/playground/machine-learning-playground/ml/lib/python3.12/site-packages (2.16.0)\n",
      "Requirement already satisfied: tensorflow<2.17,>=2.16 in /Users/spaccs01/workspace/playground/machine-learning-playground/ml/lib/python3.12/site-packages (from tf-keras) (2.16.2)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/spaccs01/workspace/playground/machine-learning-playground/ml/lib/python3.12/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/spaccs01/workspace/playground/machine-learning-playground/ml/lib/python3.12/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /Users/spaccs01/workspace/playground/machine-learning-playground/ml/lib/python3.12/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (25.1.24)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/spaccs01/workspace/playground/machine-learning-playground/ml/lib/python3.12/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/spaccs01/workspace/playground/machine-learning-playground/ml/lib/python3.12/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /Users/spaccs01/workspace/playground/machine-learning-playground/ml/lib/python3.12/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (3.12.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/spaccs01/workspace/playground/machine-learning-playground/ml/lib/python3.12/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /Users/spaccs01/workspace/playground/machine-learning-playground/ml/lib/python3.12/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/spaccs01/workspace/playground/machine-learning-playground/ml/lib/python3.12/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: packaging in /Users/spaccs01/workspace/playground/machine-learning-playground/ml/lib/python3.12/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Users/spaccs01/workspace/playground/machine-learning-playground/ml/lib/python3.12/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (4.25.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/spaccs01/workspace/playground/machine-learning-playground/ml/lib/python3.12/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /Users/spaccs01/workspace/playground/machine-learning-playground/ml/lib/python3.12/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (75.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/spaccs01/workspace/playground/machine-learning-playground/ml/lib/python3.12/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/spaccs01/workspace/playground/machine-learning-playground/ml/lib/python3.12/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/spaccs01/workspace/playground/machine-learning-playground/ml/lib/python3.12/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/spaccs01/workspace/playground/machine-learning-playground/ml/lib/python3.12/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/spaccs01/workspace/playground/machine-learning-playground/ml/lib/python3.12/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (1.70.0)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in /Users/spaccs01/workspace/playground/machine-learning-playground/ml/lib/python3.12/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in /Users/spaccs01/workspace/playground/machine-learning-playground/ml/lib/python3.12/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (3.8.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in /Users/spaccs01/workspace/playground/machine-learning-playground/ml/lib/python3.12/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/spaccs01/workspace/playground/machine-learning-playground/ml/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow<2.17,>=2.16->tf-keras) (0.45.1)\n",
      "Requirement already satisfied: rich in /Users/spaccs01/workspace/playground/machine-learning-playground/ml/lib/python3.12/site-packages (from keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras) (13.9.4)\n",
      "Requirement already satisfied: namex in /Users/spaccs01/workspace/playground/machine-learning-playground/ml/lib/python3.12/site-packages (from keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras) (0.0.8)\n",
      "Requirement already satisfied: optree in /Users/spaccs01/workspace/playground/machine-learning-playground/ml/lib/python3.12/site-packages (from keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/spaccs01/workspace/playground/machine-learning-playground/ml/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf-keras) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/spaccs01/workspace/playground/machine-learning-playground/ml/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf-keras) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/spaccs01/workspace/playground/machine-learning-playground/ml/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf-keras) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/spaccs01/workspace/playground/machine-learning-playground/ml/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf-keras) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/spaccs01/workspace/playground/machine-learning-playground/ml/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf-keras) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/spaccs01/workspace/playground/machine-learning-playground/ml/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/spaccs01/workspace/playground/machine-learning-playground/ml/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf-keras) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/spaccs01/workspace/playground/machine-learning-playground/ml/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf-keras) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/spaccs01/workspace/playground/machine-learning-playground/ml/lib/python3.12/site-packages (from rich->keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/spaccs01/workspace/playground/machine-learning-playground/ml/lib/python3.12/site-packages (from rich->keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/spaccs01/workspace/playground/machine-learning-playground/ml/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tf-keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de34fa2-c4b7-438d-b6a9-6f70c873e328",
   "metadata": {},
   "source": [
    "## Step 2: Import Libraries\n",
    "\n",
    "Import the required libraries for the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e62f52e4-1226-4fca-b3cd-2caf1d8ceaa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-22 15:35:52.490537: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFAutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72cdcf2-a799-483e-812c-23fdce6edc63",
   "metadata": {},
   "source": [
    "## Step 3: Load Pre-trained Model and Tokenizer\n",
    "\n",
    "Load a pre-trained model and tokenizer from Hugging Face. We'll use the GPT-2 model for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1532eead-f562-404a-a0e3-f0c60af37900",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = TFAutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49df662d-d0b9-4972-bb27-4bbc8367f7de",
   "metadata": {},
   "source": [
    "## Step 4: Prepare the Dataset\n",
    "\n",
    "Load the Harry Potter books dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f387cc4a-8d24-4e51-845a-fdc07a0d57a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = load_dataset('text', data_files={'train': 'harry_potter_books.txt'})\n",
    "\n",
    "with open('harrypotter.txt', 'r') as file:\n",
    "    data = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8c0d06-3bd7-4ed0-8cce-bf98ab6788f7",
   "metadata": {},
   "source": [
    "## Step 5: Tokenize the Dataset\n",
    "\n",
    "Tokenize the dataset using the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c5fc9a5-3bc0-490e-b92f-290caaa32e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenized_datasets = tokenizer(data, return_tensors='tf', truncation=True, padding='max_length', max_length=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdcf49f-6bb6-43a1-8986-ff291a44d242",
   "metadata": {},
   "source": [
    "## Step 6: Prepare Data for Training\n",
    "\n",
    "Convert the tokenized dataset into a format suitable for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6507a9c2-bb88-4ee5-b08c-5ac21e6afb6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "04d55b91-d646-4d13-8ae0-e9f6a6c93e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11df3c45-c93d-49a7-8b81-4b72854d9910",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenized_datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mtokenized_datasets\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto_tf_dataset(\n\u001b[1;32m      2\u001b[0m     columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      3\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      4\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,\n\u001b[1;32m      5\u001b[0m     collate_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m: tf\u001b[38;5;241m.\u001b[39mstack([f[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m x]),\n\u001b[1;32m      6\u001b[0m                           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m: tf\u001b[38;5;241m.\u001b[39mstack([f[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m x]),\n\u001b[1;32m      7\u001b[0m                           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m: tf\u001b[38;5;241m.\u001b[39mstack([f[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m x])}\n\u001b[1;32m      8\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenized_datasets' is not defined"
     ]
    }
   ],
   "source": [
    "train_dataset = tokenized_datasets['train'].to_tf_dataset(\n",
    "    columns=['input_ids', 'attention_mask'],\n",
    "    shuffle=True,\n",
    "    batch_size=8,\n",
    "    collate_fn=lambda x: {'input_ids': tf.stack([f['input_ids'] for f in x]),\n",
    "                          'attention_mask': tf.stack([f['attention_mask'] for f in x]),\n",
    "                          'labels': tf.stack([f['input_ids'] for f in x])}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4228c9-191c-4c30-8eb2-4cbde52808f3",
   "metadata": {},
   "source": [
    "## Step 7: Compile the Model\n",
    "\n",
    "Compile the model with an appropriate optimizer and loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efe1b02d-65bd-4a9d-b2b2-884df9a7e5a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-5\u001b[39m)\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39moptimizer, loss\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mcompute_loss)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "model.compile(optimizer=optimizer, loss=model.compute_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4baeb2-543e-40a8-8c40-c67bdc28828f",
   "metadata": {},
   "source": [
    "## Step 8: Train the Model\n",
    "\n",
    "Train the model on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa14c644-2d72-451a-9c6e-9d5ae80d7294",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mfit(train_dataset, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.fit(train_dataset, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd52c9b-bb50-476b-b2cc-2e17e729046e",
   "metadata": {},
   "source": [
    "## Step 9: Save the Model\n",
    "\n",
    "Save the fine-tuned model for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3aee61fd-3177-482e-bc71-20ed7de450da",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfine_tuned_gpt2_harry_potter\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfine_tuned_gpt2_harry_potter\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.save_pretrained('fine_tuned_gpt2_harry_potter')\n",
    "tokenizer.save_pretrained('fine_tuned_gpt2_harry_potter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a852eb6-19c8-44d3-8005-81869b979a7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
