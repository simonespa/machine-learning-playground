{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24e60102-1f23-4cf5-bf79-f030aa9fd197",
   "metadata": {},
   "source": [
    "## Some NLP Tasks\n",
    "- **Machine Translation:** Transforming text from the source language to the target language\n",
    "- **Tokenization:** Breaking text into smaller units (tokens)\n",
    "- **Part-of-Speech (POS) Tagging:** Identifying the grammatical role of each word (noun, verb, adjective, etc.)\n",
    "- **Speech-To-Text:** Translation of speech to written text\n",
    "- **Named Entity Recognition (NER):** Identifying and classifying named entities like people, organizations, and locations\n",
    "- **Syntactic Parsing:** Analyzing the grammatical structure of a sentence\n",
    "- **Coreference Resolution:** Identifying when different words or phrases refer to the same entity\n",
    "- **Sentiment Analysis:** Determining the emotional tone or attitude (positive, negative, neutral) expressed in text\n",
    "- **Topic Modeling:** Discovering abstract \"topics\" that occur in a collection of documents\n",
    "- **Topic Classification:** Categorizing documents or text segments into predefined \"topics\" (e.g. politic, sport)\n",
    "- **Information Extraction:** Extracting structured information and relationships from unstructured text\n",
    "- **Question Answering (QA):** Providing direct, relevant answers to questions posed in natural language\n",
    "- **Text Classification:** Categorizing documents or text segments into predefined classes (e.g., spam filtering)\n",
    "- **Text Summarization:** Generation of text which is the summary of the original one\n",
    "- **Text Generation:** Generation of text given a prompt (i.e. context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0126672c-d691-4fe9-ad41-36312cc2958d",
   "metadata": {},
   "source": [
    "## Word2vec\n",
    "Word2vec is built using two main pre-training tasks:\n",
    "- Continuous Bag-Of-Words (CBOW) that predicts the center word by averaging context embeddings\n",
    "- Skip-gram (SG) that predicts context words from the center word\n",
    "\n",
    "Contextual word embeddings are based on Masked Language Modeling (MLM) pre-training, while CBOW follows a rudimentary form of MLM task, predicting a masked target word from a context window without any perturbation of the masked word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca90c0f-da13-4ea3-9cd1-4eb5d98a6139",
   "metadata": {},
   "source": [
    "## BERT\n",
    "\n",
    "BERT (Bidirectional Encoder Representations from Transformers) is an open-source machine learning framework for natural language processing (NLP). Developed by Google in 2018. BERT was one of the first models to use a bidirectional approach, allowing it to understand the full context of a word by analyzing the words that precede and follow it.\n",
    "\n",
    "BERT is pre-trained through two main tasks:\n",
    "- Masked Language Modeling (MLM), where it predicts randomly masked words in a sentence by looking at the surrounding context\n",
    "- Next Sentence Prediction (NSP), where it determines if two sentences are consecutive in the original document.\n",
    "\n",
    "### Properties\n",
    "- autoencoding model\n",
    "- encoder-only architecture\n",
    "- understands the context of a sequence\n",
    "- good at language comprehension tasks\n",
    "- reconstructs masked portions of the text, using left and right context (bidirectional)\n",
    "\n",
    "## GPT\n",
    "GPT (Generative Pre-trained Transformer) is a family of large language models (LLMs) developed by OpenAI. These AI models use a deep learning architecture known as a \"transformer\" to generate human-like text and process a wide range of other data, including images, audio, and code. \n",
    "\n",
    "GPT is pre-trained through a task called next-token prediction. This is an unsupervised learning process that allows the model to develop a deep and broad understanding of language.\n",
    "\n",
    "### Properties\n",
    "- autoregressive model\n",
    "- decoder-only architecture\n",
    "- predicts the next token in a sequence\n",
    "- good at text generation\n",
    "- predicts the next token in a sequence, conditioned on the preceding tokens\n",
    "\n",
    "## T5\n",
    "T5, which stands for Text-to-Text Transfer Transformer, is a powerful series of large language models developed by Google AI. It gained prominence for its unified framework, which recasts all natural language processing (NLP) tasks into a text-to-text format. This approach simplifies the architecture and training process, allowing the same model and hyperparameters to be used for a wide variety of tasks. \n",
    "\n",
    "### Properties\n",
    "- hybrid model (uses aspects of both autoencoding and autoregressive approaches)\n",
    "- encoder-decoder architecture\n",
    "- provides a unified, versatile framework for all NLP tasks by converting them into a text-to-text format.\n",
    "- can do the same tasks as BERT and GPT by reframing the problem\n",
    "\n",
    "## What is an Encoder-Decoder?\n",
    "Encoder-decoder models can utilize autoregressive techniques, particularly in the decoder part. While the encoder typically processes the entire input sequence in parallel, the decoder often generates the output sequence token by token, relying on previously generated tokens. This sequential, token-by-token generation is the core idea behind autoregressive models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40dcd5be-c16a-4daa-b6c6-ed1eacd3201b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
