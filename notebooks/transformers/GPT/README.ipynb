{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a3e11f4-0fb5-4c01-9eb5-96fde4e2b8a8",
   "metadata": {},
   "source": [
    "# GPT\n",
    "\n",
    "Generative Pre-trained Transformers (GPT) is an autoregressive language model (generative) decoders-only architecture trained on a huge corpora of data (pre-trained) with the decoder part taken from the Transformer architecture (transformers).\n",
    "\n",
    "Autoregressive means that the model is predicting tokens with only one side of the context (the past side of the context)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c2a733-c15f-4194-9e69-bd3825d85ab4",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "GPT uses a tokenization called [byte-level Byte-Pair Encoding (BPE)](https://en.wikipedia.org/wiki/Byte-pair_encoding) which converts text to a fixed-size byte representation before creating tokens. Byte-level BPE starts with a vocabulary of 256 basic byte units, merging frequent byte sequences into new, larger tokens. This approach provides robust handling of any text, including emojis and unknown characters, by ensuring all inputs are represented, avoiding out-of-vocabulary (OOV) issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05367cf8-f589-48da-bf26-5fbd43336ee1",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "\n",
    "GPT 2 applies two separate types of embeddings to tokenized sentences:\n",
    "- Word Token Embeddings (WTE)\n",
    "  1. Represents context-free (context-less) meaning of each token\n",
    "  2. A lookup of over 50000 possible vectors\n",
    "  3. This is learnable during training\n",
    "- Word Position Embeddings (WPE)\n",
    "  1. Used to represent the token's position in the sentence\n",
    "  2. This is not learnable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf23f35-578c-46da-b4b1-2fb863f31d88",
   "metadata": {},
   "source": [
    "## Pre-training\n",
    "\n",
    "GPT-2 is pre-trained on the auto-regressive language modelling task. The corpora was a 40GB dataset worth of text, called WebText. They scraped all outbound links from Reddit that received at least 3 karma. An indicator for whether other users found the link interesting, educational or just funny. The resulting dataset contained the text subset of these 45 million links.\n",
    "\n",
    "People are doing things like translating from English to French or viceversa, or doing sentiment analysis, or other natural language processing tasks, naturally occurring in these links from Reddit. So the idea was, by training GPT on this task, the auto-regressive language modeling task, it was possible to pre-train GPT to perform multiple types of tasks at once. This helped making GPT popular, but this dataset contained a lot of bias as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163bc01f-6f17-47af-a579-bdcc9a0c21e8",
   "metadata": {},
   "source": [
    "## Few-shot Learning\n",
    "\n",
    "- **Zero-shot learning:** the model may be provided with a task description and no prior examples. The task is set up using a prompt.\n",
    "- **One-shot learning:** the model is provided with a task description and a simple example of the task.\n",
    "- **Few-shot learning:** the model is provided with a task description and as many examples as we desire / fit in the context window of the model. For GPT-2 this is 1024 tokens.\n",
    "\n",
    "The idea of few-shot learning stems from the fact that GPT-2 was trained on a corpora containing instances of people performing NLP tasks such as translation, sentiment analysis, etc. The model wasn't trained with these other tasks in mind, it was only trained with an auto-regressive modelling taks in mind, but the fact that GPT-2 may have learned how to perform other NLP tasks by simply seeing them in the pre-training text, that's what makes few-shot learning fascinating.\n",
    "\n",
    "GPT-3 paper: https://arxiv.org/abs/2005.14165"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d334805-36f8-4cce-848a-bff95be3e573",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
