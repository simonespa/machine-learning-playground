{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3760fd58-cb8c-4ebc-b1b5-70e7d6e46cd6",
   "metadata": {},
   "source": [
    "# Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c03beaa-b91a-44d7-a000-653ae2e66b9f",
   "metadata": {},
   "source": [
    "## List of deep learning optimizers\n",
    "\n",
    "Here’s an overview of key deep learning optimizers, their meanings, mechanisms, features, pros/cons, and relationships to each other:\n",
    "\n",
    "### 1. SGD (Stochastic Gradient Descent)\n",
    "- **Meaning**: Stochastic means random sampling; Gradient Descent minimizes loss by updating weights in the opposite direction of the gradient.\n",
    "- **How it works**: Updates parameters using the gradient of the loss function computed for a random subset (mini-batch) of data.\n",
    "  $$\n",
    "  \\theta = \\theta - \\eta \\cdot \\nabla L(\\theta)\n",
    "  $$\n",
    "  where $ \\eta $ is the learning rate.\n",
    "- **Key Features**: Simple, fast for large datasets, sensitive to learning rates, prone to oscillations in noisy gradients.\n",
    "- **Advantages**: Computationally efficient.\n",
    "- **Disadvantages**: Can get stuck in local minima or saddle points; convergence may be slow.\n",
    "- **Relation**: Basis for most advanced optimizers.\n",
    "\n",
    "### 2. Momentum\n",
    "- **Meaning**: Adds a \"momentum\" term to smooth updates.\n",
    "- **How it works**: Accumulates a moving average of past gradients to add inertia:\n",
    "  $$\n",
    "  v_t = \\gamma v_{t-1} + \\eta \\cdot \\nabla L(\\theta)\n",
    "  $$\n",
    "  $$\n",
    "  \\theta = \\theta - v_t\n",
    "  $$\n",
    "  where $ \\gamma $ is the momentum coefficient.\n",
    "- **Key Features**: Speeds up convergence in relevant directions; reduces oscillations.\n",
    "- **Advantages**: Improves SGD by making it more stable and faster.\n",
    "- **Disadvantages**: Requires tuning of momentum coefficient.\n",
    "- **Relation**: Builds on SGD.\n",
    "\n",
    "### 3. Nesterov Accelerated Gradient (NAG)\n",
    "- **Meaning**: Extension of momentum that \"looks ahead\" for the gradient at the anticipated position.\n",
    "- **How it works**: Computes gradient at a future position:\n",
    "  $$\n",
    "  \\theta_{lookahead} = \\theta - \\gamma v_{t-1}\n",
    "  $$\n",
    "  $$\n",
    "  v_t = \\gamma v_{t-1} + \\eta \\nabla L(\\theta_{lookahead})\n",
    "  $$\n",
    "  $$\n",
    "  \\theta = \\theta - v_t\n",
    "  $$\n",
    "- **Key Features**: Reduces overshooting and improves convergence.\n",
    "- **Advantages**: Faster convergence than momentum.\n",
    "- **Disadvantages**: Slightly more complex; still requires tuning.\n",
    "- **Relation**: Builds on momentum.\n",
    "\n",
    "### 4. Adagrad (Adaptive Gradient Algorithm)\n",
    "- **Meaning**: Adaptive learning rates for each parameter.\n",
    "- **How it works**: Scales learning rate inversely with the sum of squared gradients:\n",
    "  $$\n",
    "  \\theta = \\theta - \\frac{\\eta}{\\sqrt{G_t + \\epsilon}} \\cdot \\nabla L(\\theta)\n",
    "  $$\n",
    "  where $ G_t $ is the sum of past squared gradients, and $ \\epsilon $ avoids division by zero.\n",
    "- **Key Features**: Larger updates for infrequent parameters; smaller updates for frequent ones.\n",
    "- **Advantages**: No need to manually tune learning rates.\n",
    "- **Disadvantages**: Learning rate decays too aggressively, leading to vanishing updates.\n",
    "- **Relation**: Improves upon SGD by adapting learning rates.\n",
    "\n",
    "### 5. RMSProp (Root Mean Squared Propagation)\n",
    "- **Meaning**: Adapts Adagrad by using a moving average of squared gradients.\n",
    "- **How it works**: Computes a decaying average of squared gradients:\n",
    "  $$\n",
    "  E[g^2]_t = \\gamma E[g^2]_{t-1} + (1 - \\gamma) \\nabla L(\\theta)^2\n",
    "  $$\n",
    "  $$\n",
    "  \\theta = \\theta - \\frac{\\eta}{\\sqrt{E[g^2]_t + \\epsilon}} \\cdot \\nabla L(\\theta)\n",
    "  $$\n",
    "- **Key Features**: Solves Adagrad's aggressive decay problem.\n",
    "- **Advantages**: Better for nonstationary problems; works well with RNNs.\n",
    "- **Disadvantages**: Requires tuning of decay rate $ \\gamma $.\n",
    "- **Relation**: Improves upon Adagrad.\n",
    "\n",
    "### 6. Adam (Adaptive Moment Estimation)\n",
    "- **Meaning**: Combines momentum and RMSProp for adaptive learning rates and stability.\n",
    "- **How it works**: Maintains exponentially decaying averages of gradients and squared gradients:\n",
    "  $$\n",
    "  m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) \\nabla L(\\theta)\n",
    "  $$\n",
    "  $$\n",
    "  v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) \\nabla L(\\theta)^2\n",
    "  $$\n",
    "  Bias corrections are applied:\n",
    "  $$\n",
    "  \\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}, \\quad \\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}\n",
    "  $$\n",
    "  Update:\n",
    "  $$\n",
    "  \\theta = \\theta - \\frac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon} \\cdot \\hat{m}_t\n",
    "  $$\n",
    "- **Key Features**: Adaptive and robust.\n",
    "- **Advantages**: Efficient; works well for most applications.\n",
    "- **Disadvantages**: Can converge too quickly to suboptimal solutions.\n",
    "- **Relation**: Combines Momentum and RMSProp.\n",
    "\n",
    "### 7. AdaMax\n",
    "- **Meaning**: Variant of Adam using $ L_\\infty $-norm for updates.\n",
    "- **How it works**: Similar to Adam, but replaces $ \\hat{v}_t $ with the maximum absolute gradient observed:\n",
    "  $$\n",
    "  u_t = \\max(\\beta_2 u_{t-1}, |\\nabla L(\\theta)|)\n",
    "  $$\n",
    "- **Key Features**: More stable for large gradients.\n",
    "- **Advantages**: Works better in some high-dimensional spaces.\n",
    "- **Disadvantages**: Less commonly used.\n",
    "- **Relation**: Variant of Adam.\n",
    "\n",
    "### 8. Nadam (Nesterov-accelerated Adam)\n",
    "- **Meaning**: Combines Adam with Nesterov momentum.\n",
    "- **How it works**: Adds a look-ahead step similar to NAG to Adam's updates.\n",
    "- **Key Features**: Further improves Adam's convergence.\n",
    "- **Advantages**: Slightly faster convergence.\n",
    "- **Disadvantages**: Slightly more complex than Adam.\n",
    "- **Relation**: Combines Adam and NAG.\n",
    "\n",
    "---\n",
    "\n",
    "### 9. AMSGrad\n",
    "- **Meaning**: Modification of Adam to avoid the \"non-convergence\" issue by using a maximum of past squared gradients.\n",
    "- **How it works**: Ensures $ v_t $ never decreases:\n",
    "  $$\n",
    "  v_t = \\max(v_{t-1}, \\hat{v}_t)\n",
    "  $$\n",
    "- **Key Features**: Improves theoretical convergence guarantees.\n",
    "- **Advantages**: More robust than Adam.\n",
    "- **Disadvantages**: Slightly slower in practice.\n",
    "- **Relation**: Variant of Adam.\n",
    "\n",
    "### 10. LAMB (Layer-wise Adaptive Moments)\n",
    "- **Meaning**: Designed for large-batch training by scaling Adam with weight norms.\n",
    "- **How it works**: Scales updates with layer-wise weight norms:\n",
    "  $$\n",
    "  r_t = \\frac{\\|\\theta_t\\|}{\\|\\Delta \\theta_t\\|}\n",
    "  $$\n",
    "- **Key Features**: Scales better with large batches.\n",
    "- **Advantages**: Optimized for large-scale systems like transformers.\n",
    "- **Disadvantages**: Requires more tuning.\n",
    "- **Relation**: Extends Adam for large-scale problems.\n",
    "\n",
    "### Relationships Between Optimizers\n",
    "- **SGD**: Base algorithm.\n",
    "- **Momentum**: Smooths SGD updates.\n",
    "- **NAG**: Improves momentum.\n",
    "- **Adagrad**: Adapts learning rates for SGD.\n",
    "- **RMSProp**: Fixes Adagrad’s decay issue.\n",
    "- **Adam**: Combines RMSProp and momentum.\n",
    "- **Nadam**: Combines Adam with NAG.\n",
    "- **AMSGrad**: Stabilizes Adam for convergence.\n",
    "- **AdaMax**: Variant of Adam using a different norm.\n",
    "- **LAMB**: Extends Adam for scalability.\n",
    "\n",
    "### Summary Table:\n",
    "| Optimizer  | Features                    | Advantages                  | Disadvantages             |\n",
    "|------------|-----------------------------|-----------------------------|---------------------------|\n",
    "| SGD        | Simple, efficient           | Computationally cheap       | Sensitive to tuning       |\n",
    "| Momentum   | Smooths SGD updates         | Speeds up convergence       | Requires $ \\gamma $       |\n",
    "| NAG        | Adds lookahead              | Reduces overshooting        | Slightly complex          |\n",
    "| Adagrad    | Adapts learning rates       | No manual tuning            | Aggressive decay          |\n",
    "| RMSProp    | Fixes Adagrad decay         | Good for RNNs               | Needs decay rate tuning   |\n",
    "| Adam       | Combines RMSProp/Momentum   | Adaptive, robust            | May overfit or converge poorly |\n",
    "| AMSGrad    | Stabilizes Adam             | Better theoretical guarantees | Slower in practice        |\n",
    "| Nadam      | Nesterov + Adam             | Faster convergence          | Complex implementation    |\n",
    "| AdaMax     | Adam with $ L_\\infty $-norm | Better in some cases        | Rarely used               |\n",
    "| LAMB       | Layer-wise adaptive scaling | Great for large-scale models | Complex, needs tuning     |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf91c6fc-253e-4dd6-b7e6-ace3bef8f53f",
   "metadata": {},
   "source": [
    "## Differences Between SGD, Batch SGD, and Mini-Batch SGD\n",
    "\n",
    "### 1. SGD (Stochastic Gradient Descent)\n",
    "- **Meaning**: Updates parameters using gradients computed for a **single sample** at a time.\n",
    "- **How it works**: For each training example $ x^{(i)} $, compute the gradient and update the model:\n",
    "  $$\n",
    "  \\theta = \\theta - \\eta \\cdot \\nabla L(\\theta; x^{(i)})\n",
    "  $$\n",
    "  where $ L(\\theta; x^{(i)}) $ is the loss for the individual sample.\n",
    "- **Key Features**:\n",
    "  - Updates the model after every single data point.\n",
    "  - Introduces randomness (stochastic) into the updates.\n",
    "- **Advantages**:\n",
    "  - Faster updates since they are computed for a single sample.\n",
    "  - Can escape saddle points and local minima due to noisy gradients.\n",
    "- **Disadvantages**:\n",
    "  - Noisy updates may lead to unstable convergence.\n",
    "  - Less efficient for parallel computation.\n",
    "- **Relation**:\n",
    "  - Simplest form of gradient descent.\n",
    "  - Forms the basis for Mini-Batch and Batch SGD.\n",
    "\n",
    "### 2. Batch Gradient Descent\n",
    "- **Meaning**: Updates parameters using gradients computed on the **entire dataset**.\n",
    "- **How it works**: Compute the gradient of the loss function for all $ N $ samples in the dataset and then update:\n",
    "  $$\n",
    "  \\theta = \\theta - \\eta \\cdot \\frac{1}{N} \\sum_{i=1}^{N} \\nabla L(\\theta; x^{(i)})\n",
    "  $$\n",
    "- **Key Features**:\n",
    "  - Deterministic updates, as the gradient is computed over the entire dataset.\n",
    "  - Requires all training data to be loaded into memory.\n",
    "- **Advantages**:\n",
    "  - More stable updates; smooth convergence.\n",
    "  - Suitable for convex loss functions.\n",
    "- **Disadvantages**:\n",
    "  - Computationally expensive for large datasets.\n",
    "  - Requires significant memory for large datasets.\n",
    "  - Slow, as updates occur only after processing the entire dataset.\n",
    "- **Relation**:\n",
    "  - A baseline for comparison; Mini-Batch SGD improves upon Batch SGD.\n",
    "\n",
    "### 3. Mini-Batch SGD\n",
    "- **Meaning**: Updates parameters using gradients computed for a **subset (mini-batch)** of the dataset.\n",
    "- **How it works**: Divide the dataset into $ M $ mini-batches (typically 16–512 samples). For each mini-batch $ B_k $:\n",
    "  $$\n",
    "  \\theta = \\theta - \\eta \\cdot \\frac{1}{|B_k|} \\sum_{x \\in B_k} \\nabla L(\\theta; x)\n",
    "  $$\n",
    "  where $ |B_k| $ is the mini-batch size.\n",
    "- **Key Features**:\n",
    "  - Combines the stability of Batch SGD with the efficiency of SGD.\n",
    "  - Parallelizable across GPUs or CPUs.\n",
    "- **Advantages**:\n",
    "  - Faster convergence compared to Batch SGD.\n",
    "  - Reduces noise in updates compared to SGD.\n",
    "  - Works efficiently for large datasets.\n",
    "- **Disadvantages**:\n",
    "  - Requires tuning of mini-batch size.\n",
    "  - May still introduce some noise into updates.\n",
    "- **Relation**:\n",
    "  - Balances between SGD and Batch SGD, improving computational efficiency and convergence.\n",
    "\n",
    "### Relationships Between Optimizers\n",
    "| **Optimizer**       | **Updates Per**             | **Key Features**                | **Advantages**              | **Disadvantages**            |\n",
    "|----------------------|-----------------------------|----------------------------------|-----------------------------|------------------------------|\n",
    "| **SGD**             | Single sample               | Randomness in updates           | Fast updates; escapes minima | Noisy and unstable updates   |\n",
    "| **Batch SGD**        | Entire dataset             | Deterministic, stable           | Smooth convergence          | High memory and time cost    |\n",
    "| **Mini-Batch SGD**   | Mini-batch (subset)        | Balance of speed and stability  | Efficient and scalable      | Some noise in updates        |\n",
    "\n",
    "### Summary of Relationships\n",
    "- **SGD**: Simplest, fastest updates, but highly noisy.\n",
    "- **Batch SGD**: Smooth but computationally slow; impractical for large datasets.\n",
    "- **Mini-Batch SGD**: A middle ground, combining the efficiency of SGD with the stability of Batch SGD. Most commonly used in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68711ff7-faba-410a-9da4-cd6b4ce82db8",
   "metadata": {},
   "source": [
    "## Are Batch SGD and Gradient Descent the same thing?\n",
    "\n",
    "**Batch SGD** is effectively the same as **Gradient Descent** (often referred to as **Batch Gradient Descent**) in most contexts. Here’s a more detailed comparison:\n",
    "\n",
    "### Gradient Descent (Batch Gradient Descent)\n",
    "- **Meaning**: Computes the gradient for the **entire dataset** at once and updates the model parameters accordingly.\n",
    "- **Why It's Called Batch SGD**: The \"batch\" refers to using the **entire dataset** as a single batch for computing the gradient.\n",
    "- **How it Works**: \n",
    "  $$\n",
    "  \\theta = \\theta - \\eta \\cdot \\frac{1}{N} \\sum_{i=1}^{N} \\nabla L(\\theta; x^{(i)})\n",
    "  $$\n",
    "  - $ N $: Total number of samples in the dataset.\n",
    "  - The gradient is averaged over the entire dataset before making an update.\n",
    "\n",
    "### Differences Between Batch SGD and SGD (Stochastic Gradient Descent)\n",
    "| **Aspect**         | **Gradient Descent (Batch SGD)** | **Stochastic Gradient Descent (SGD)** |\n",
    "|---------------------|----------------------------------|----------------------------------------|\n",
    "| **Data Processed**  | Entire dataset at once          | One sample at a time                  |\n",
    "| **Update Frequency**| Once per epoch                  | After each sample                     |\n",
    "| **Stability**       | Stable and deterministic        | Noisy and stochastic                  |\n",
    "| **Efficiency**      | Computationally expensive       | Fast updates but less stable          |\n",
    "| **Convergence**     | Smooth convergence              | Can oscillate but escapes minima       |\n",
    "\n",
    "### Why the Term \"Batch SGD\" Exists\n",
    "- Historically, **\"Gradient Descent\"** referred to processing the entire dataset (batch) at once.\n",
    "- With the rise of **Mini-Batch SGD** (where smaller subsets are used) and **SGD** (one-sample updates), the term \"Batch SGD\" emerged to distinguish **Gradient Descent** as the method that uses the entire dataset for updates.\n",
    "\n",
    "### Practical Context\n",
    "In modern machine learning, **Batch Gradient Descent** (or Batch SGD) is rarely used because:\n",
    "1. It is computationally expensive for large datasets.\n",
    "2. It requires the entire dataset to fit in memory.\n",
    "3. It updates parameters only once per epoch, making it slower to react to gradient changes.\n",
    "\n",
    "Instead, **Mini-Batch SGD** is preferred for balancing computational efficiency and convergence stability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d582dc51-fef1-4c52-bd0a-541c96f9645b",
   "metadata": {},
   "source": [
    "## References\n",
    "- https://towardsdatascience.com/understanding-deep-learning-optimizers-momentum-adagrad-rmsprop-adam-e311e377e9c2\n",
    "- https://towardsdatascience.com/a-visual-explanation-of-gradient-descent-methods-momentum-adagrad-rmsprop-adam-f898b102325c"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
