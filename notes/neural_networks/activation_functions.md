# Activation Functions

Also known as **Transfer Function**. Activation functions can be linear (a.k.a. Identity) and non-linear. The non-linear type is the most used because it helps with generalisation.

The non linear 

Sigmoid, tanh, Softmax, ReLU, Leaky ReLU
