{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b567050-931b-403f-9b08-c2875fd77c48",
   "metadata": {},
   "source": [
    "# RetNet\n",
    "\n",
    "Paper: https://arxiv.org/pdf/2307.08621.pdf\n",
    "\n",
    "<img src=\"./img/impossible_triangle.png\" alt=\"Impossible Triangle\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d02ec1a-8bca-4ee1-ac1d-92909a7f8c20",
   "metadata": {},
   "source": [
    "## Perplexity\n",
    "\n",
    "It is an intrinsic evaluation metric of performances for language model in Natural Language Processing.\n",
    "\n",
    "It measure - on average - how many different equally most probable words can follow any given word.\n",
    "\n",
    "Lower perplexities represent better language models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3993c04f-af2d-4015-bfee-12fa9965779c",
   "metadata": {},
   "source": [
    "## Performances comparison with Transformers\n",
    "\n",
    "From the paper\n",
    "\n",
    "> Retentive network (RetNet) achieves low-cost inference (i.e., GPU memory, throughput,\n",
    "and latency), training parallelism, and favorable scaling curves compared with Transformer. Results\n",
    "of inference cost are reported with 8k as input length. Figure 6 shows more results on different\n",
    "sequence lengths.\n",
    "\n",
    "<img src=\"./img/performances_comparison.png\" alt=\"Impossible Triangle\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26c8605-f30e-4e6a-b48f-7ae2cbbf3d6b",
   "metadata": {},
   "source": [
    "## Parallelism\n",
    "\n",
    "Transformers use the self-attention mechanism which is highly parallelisable. During training it's an advantage, but during inference time it's a drawback."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb06cd51-78c7-4f16-bc88-3a15e8ceed8f",
   "metadata": {},
   "source": [
    "## Inference cost & memory complexity\n",
    "\n",
    "- Inference cost per time step refers to GPU memory, throughput and latency\n",
    "- Memory complexity refers to the scaling laws of the memory footprint with respect to sequence length.\n",
    "\n",
    "### RNNs\n",
    "\n",
    "RNNs use matrix multiplications.\n",
    "\n",
    "- The inference time complexity is constant $O(1)$\n",
    "- The inference memory complexity is linear $O(N)$\n",
    "\n",
    "### Transformers\n",
    "\n",
    "Transformers use the self-attention mechanism, possibly using a Multi-Head Attention approach, they need to store in memory the $N \\times N$ to be used during inference.\n",
    "\n",
    "- The inference time complexity is linear $O(N)$\n",
    "- The inference memory complexity is quadratic $O(N^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ccee2e-96c3-4cc5-81b4-6e89ff6a60e7",
   "metadata": {},
   "source": [
    "### Improvements\n",
    "\n",
    "- Use of self-attention during training to achieve parallelism\n",
    "- The self-attention mechanism is replaced with the so-called **retention mechanism** combined with the **recurrent inference** approach. This approach improves upon the self-attention during training.\n",
    "- RetNet introduces a **Multi-Scale retention mechanism** which replaces the **Multi-Head Attention mechanism**. This helps reducing the complexity at inference time.\n",
    "\n",
    "- **Parallel approach**: used during training\n",
    "- **Recurrent approach**: used during inference to reduce the time/memory complexity to constant/linear\n",
    "- **Chunkwise recurrent approach**: performs efficient modelling of long sequences. Each local block is encoded in parallel, then each local block is subsequently encoded in a recurrent fashion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fba00f-6ab9-465c-8caa-2f8f5f8a01d3",
   "metadata": {},
   "source": [
    "## Self-Attention\n",
    "\n",
    "## SoftMax\n",
    "\n",
    "RetNet replaces the SoftMax module with the [**Hadamard multiplication**](../math_recall/linear_algebra.ipynb) by a newly introduced **D-Matrix** followed by a [**Group Normalization**](https://arxiv.org/pdf/1803.08494.pdf) operation.\n",
    "\n",
    "## Normalization\n",
    "\n",
    "The step of normalization typically consist of **centering** the data around the origin and then **scaling** them so that their distribution falls within a well known multivariate normal distribution for better separability hence performances.\n",
    "\n",
    "If this works for classification, then it should work in a neural network setting too. The idea is to do this at each layer so that the next layer will receive a normalised signal.\n",
    "\n",
    "In order to normalize, you need to calculate the mean and standard deviation.\n",
    "\n",
    "<img src=\"./img/normalization.png\" alt=\"Normalization\" width=\"80%\">\n",
    "\n",
    "> Normalization methods. Each subplot shows a feature map tensor, with N as the batch axis, C as the channel axis, and (H, W)\n",
    "as the spatial axes. The pixels in blue are normalized by the same mean and variance, computed by aggregating the values of these pixels.\n",
    "\n",
    "### Batch Normalization\n",
    "\n",
    "**Batch Normalization** calculates the mean and standard deviation of a subset (batch) of data. The bigger is the batch, the better is the estimation.\n",
    "\n",
    "The problem with this mechanism lately is that the size of the input, especially with LLM has become really big, so even a batch can be quite unmanageable. You could try to split the batch itself to different parallel units, but then you will calculate the statistics on very few datapoint, which will inevitably result in poor data quality.\n",
    "\n",
    "The BatchNorm error grows with the reduction of number of samples.\n",
    "\n",
    "**BatchNorm** normalizes across the $N$ datapoints.\n",
    "\n",
    "### Group Normalization\n",
    "\n",
    "It is similar to the batch normalization process: centering and scaling. The difference is that it does the normalization without relying on the batch statistics.\n",
    "\n",
    "It works one datapoint at a time, and spans across few features. It takes the good intuitions of both Layer and Instance norms, by standardising across the features (multiple features together) but not all of them and not one at a time. The features selected may share a similar scale.\n",
    "\n",
    "GroupNorm is scale-invariant, which means it improves the numerical precision of the retention layers without affecting the outputs and the backward gradients. Basically, it does not affect the final results while stabilizing the numerical flow of both forward and backward passes, because of the scale-invariant property."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc6af98-1fcd-4fd0-9f95-1e8380de74b8",
   "metadata": {},
   "source": [
    "## Softmax, D-matrix and GroupNorm\n",
    "\n",
    "The **SoftMax** function in Transformers serves two purposes:\n",
    "- Gives models the percentage of contribution in the encoding of each single token within the sequence.\n",
    "- Introduces non-linearity\n",
    "\n",
    "By removing SoftMax, we need to replace it with something that can provide these two properties.\n",
    "\n",
    "The proposed **D-matrix** takes care of the \"attending\" part, by means of causal masking which prevents look-ahead and weights the past tokens with an exponential decay. The assumption is that more recent tokens in the sequence (closer to the current masked token) are exponentially more important than the farthest ones.\n",
    "\n",
    "> **Causal masking** is used to prevent the model from attending to future tokens during training, so that it can only rely on information from past tokens to predict the next token.\n",
    "\n",
    "In comparison, SoftMax is more flexible but has an expensive computation, while the D-matrix is less flexible - having a pre-defined weighting of tokens - but it gives a more efficient $O(1)$ inference time with a $O(N)$ memory complexity.\n",
    "\n",
    "The **GroupNorm** operation re-introduces non-linearity back."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c8e9ce-a711-4b14-98be-4dff72e0f316",
   "metadata": {},
   "source": [
    "## Retention Mechanism\n",
    "\n",
    "The **retention mechanism** combines the benefit of RNNs and Transformers. Not only that, the name itself is a combination of both: **RE**current + self-at**TENTION."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86dc910e-8352-4b64-8171-b357d9dac407",
   "metadata": {},
   "source": [
    "## Achievement\n",
    "\n",
    "- Parallel retention mechanism during training\n",
    "- Recurrent retention mechanism during inference\n",
    "\n",
    "The recurrent version of the retention mechanism is achieved by deconstructing the parallel version. It has slightly unintuitive matrix operations, but both the parallel and recurrent output are the same."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
