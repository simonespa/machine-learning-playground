# Dropout and Regularization

Dropout and regularization are techniques used to reduce the complexity and variance of the model.

Dropout is a method of randomly dropping out some units or connections in the model during training, which forces the model to learn from different subsets of the data.

Regularization is a method of adding some penalty terms to the loss function, which prevents the model from learning too much from the noise or outliers in the data.
